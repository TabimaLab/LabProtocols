---
title: "Data Analysis"
output: html_notebook
---

```{r}
library(dada2)
library(Biostrings)
library(ShortRead)
library(tidyverse)
```

## Starting info

Location of files:    mdelcogliano@smaug:~/Tabima_lab/raw_data/watershed/usftp21.novogene.com$

`zcat filename | head` will allow you to open the .gz file

read DADA2 tutorial

find the number of sequences in each file by grepping the name of the file prefaced by an @ symbol

resolution: can you figure out what is what?
  at a finer scale, the resolution of 16s is not as good as ITS bc it has to evolve much slower
  
"dont try to out-compete me on loving things" - Javier


### Listing the files

This code creates a list of the files that come from the illumina 
```{r}
path <- "/Tabima_lab/raw_data/watershed/fecal/usftp21.novogene.com/Rawdata" ## CHANGE ME to the directory containing the fastq file
list.files(path)

fnFs <- sort(list.files(path, pattern = "_1.fq.gz$", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_2.fq.gz$", full.names = TRUE))
```



```{r}
FWD <- "GGAAGTAAAAGTCGTAACAAGG"
REV <- "GCTGCGTTCTTCATCGATGC"

allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer) # The Biostrings works w DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna),
      RevComp = reverseComplement(dna))
  return(sapply(orients, toString)) # Convert back to character vector
}

FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)

FWD.orients
```

```{r}
fnFs.filtN <- file.path("~/filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path("~/filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)
```
# Step 4. Counting and removing primers

```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]),
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]),
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]),
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

cutadapt <- "/usr/bin/cutadapt" # CHANGE ME to the cutadapt path on your machine

path.cut <- file.path("~/filtN/", "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))
FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC)
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC)
```
The next step is different than in the guide. The idea is to send all the jobs to the cluster for them to execute in the background:
```{r, eval=FALSE}
for(i in seq_along(fnFs)) {
  jobname <- basename(fnFs[i]) %>% gsub(pattern = "_.+", replacement = "_cutadapt", perl = T)
  cut.sh <- paste(cutadapt, R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])
system(paste('sbatch -J', jobname ,'--wrap "', cut.sh , '"'))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]),
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]),
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]),
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
```

Where we left 2.22.23

```{r}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_1.fq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_2.fq.gz", full.names = TRUE))
```

Now we are going to create a character vector so that when we type sample.name, it gives us only the name of the sample and not the true file name. This assumes all files are in the same format.

```{r}
# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
```

Assigning the filenames for the output of the filtered reads to be stored as fastq.gz files.

**I don't get what this is doing**

```{r}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
```

Now filter and trim the reads.

**Q**: Is the above command just creating a path so that the output of whatever happens to the cutFs/cutRs lists are saved as fastq.gz files?

Out, shows how many reads were present before and how many are present after filtering and trimming.

```{r}
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 2),
    truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
head(out)
```

## Step 8: Error Rates

```{r}
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
plotErrors(errF, nominalQ = TRUE)
```

## Step 9: Dereplicate reads

Use the program `derepFastq` to dereplicate the `filtFs/filtRs`

```{r}
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
```

Name the derep-class objects by the sample names

```{r}
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

## Step 10: Sample Inference

Sample inference: in the case of sequencing analysis, as used in DADA2, it infers sequences exactly and resolves differences of as little as 1 nucleotide. This allows for less ASVs because if there is only 1 difference between sequences, other programs might count these as two different variants. By resolving these small differences, fewer spurious sequences are outputted.

For code: dada applies the core sample inference algorithm to the de-replicated list of forward and reverse reads, and saves it as `dadaFs/dadaRs`

```{r}
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)
```

## Step 11: Merge the paired reads

**Q**: Why are we merging the dadaFs and derepFs when the derepFs have not been inferred?

```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
```

## Step 12: Construct Sequence Table

This takes this list `mergers` and uses the command `makeSequenceTable` and saves it as `seqtab` dim(seqtab) shows the number of columns and rows in seqtab

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

[1]    73 21396

## Step 13: Remove Chimeras

Chimeras are artifact sequences formed by two or more biological sequences incorrectly joined together. This can leaded to inflated measures of diversity and bias population genetic parameters. Identified as short sequence fragments that are uncommon within a reference phylogenetic group. 

**Q**: Does this take the consensus of all reads present in the analysis and use this a the reference for the short fragments that are uncommon?

Use command `removeBimeraDenovo` for `seqtab` matrix, creating a consensus as the reference for chimera identification and save as `seqtab.nochim`

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
```

## Step 14:  Track the reads through the pipeline

This creates a table displaying the number of reads that were present at each step of the pipeline, so that we can keep track of how many were lost at each step.

**Q**: what is the last two lines of this code? Why are SBFM pulled out?

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers,
    getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace
# sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged",
    "nonchim")
rownames(track) <- sample.names
head(track)

rownames(seqtab.nochim)[rownames(seqtab.nochim) %in% "SBFM"] <- c("SBFM1","SBFM10","SBFM9")
saveRDS(seqtab.nochim, file = "nochimera.RDS")
```

## Step 15: Assign the Taxonomy

`unite.ref` is an object that contains a database of fungal taxonomy references. `taxa` object is defined by the function `assigntaxonomy` which takes the sequance table without chimeras, and the reference fungal sequences. tryRC means that the reverse compliment will be used to assugn taxonomy if it is a better match than the forward sequence.
`saveRDS` writes a single R object to a file. So it saves `taxa` into a new file called `taxa.RDS`.


```{r}
unite.ref <- "/Tabima_lab/homes/mhincher/Tabima_lab/raw_data/watershed/fecal/sh_general_release_dynamic_16.10.2022.fasta"  # CHANGE ME to location on your machine
taxa <- assignTaxonomy(seqtab.nochim, unite.ref, multithread = TRUE, tryRC = TRUE)
saveRDS(taxa, file = "taxa.RDS")
View(taxa)
```

DADA gives us everything above

Phloseq is the next step to transfer into asvs

Phyloseq
  - location of samples, treatment
  
object from R

seqtab:table of sequences that are nonchimeric
 - only unique sequences because chimeras are not present

otu vs asv

```{r}


```