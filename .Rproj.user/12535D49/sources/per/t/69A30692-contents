---
title: "DADA2_ITS_Pipeline"
output: html_document
date: '2022-09-29'
editor_options: 
  chunk_output_type: console
---
https://benjjneb.github.io/dada2/ITS_workflow.html

```{r}
library(dada2)
library(ShortRead)
library(Biostrings)
path <- "/Smaug_SSD/BIOL209/mhincher/Desktop/ITS_DADA2"
```

1. Generate a matched list of the forward and reverse files, and parse out the sample names. Assuming that the files are listed in the format, `samplename_1.fasta.gz` 

```{r}
fnFs <- sort(list.files(path, pattern = "_1.fastq.gz$", full.names = TRUE))
fnFs
fnRs <- sort(list.files(path, pattern = "_2.fastq.gz$", full.names = TRUE))
fnRs
```


## Identify Primers 

Primers used in sequencing 
  - Forward primer: BITS3
  - Reverse primer: B58S3
These primers were used to amplify this data set. We record the DNA sequences, including ambiguous nucleotides, for those primers.

name the forward and reverse sequences
```{r} 
FWD <- "ACCTGCGGARGGATCA"
REV <- "GAGATCCRTTGYTRAAAGTT" 
```

verify the presence and orientation of these primers in the data

```{r}
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```

Pre-filter the sequences to remove `n's` as this makes it difficult to get accurate short primer mapping of the reads

```{r} 
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)
```

We are now ready to **count the number of times the primers appear in the forward and reverse read**, while considering all possible primer orientations. Identifying and counting the primers on one set of paired end FASTQ files is sufficient, assuming all the files were created using the same library preparation, so we’ll just process the first sample.

```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```
As expected, the FWD primer is found in the forward reads in its forward orientation, and in some of the reverse reads in its reverse-complement orientation (due to read-through when the ITS region is short). Similarly the REV primer is found with its expected orientations.

## Remove Primers

```{r} 
cutadapt <- "/Tabima_lab/homes/mhincher/miniconda3/envs/cutadaptenv/bin/cutadapt" # CHANGE ME to the cutadapt path on your machine

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
```
# DADA processing

## Reading in the cut files:

```{r}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_1.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_2.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
```

## Checking QC

```{r}
plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[1:2])
```


## Filter and Trim 

Assigning the filenames for the output of the filtered reads to be stored as fastq.gz files.

``` {r}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
```

For this dataset, we will use standard filtering paraments: maxN=0 (DADA2 requires sequences contain no Ns), truncQ = 2, rm.phix = TRUE and maxEE=2. The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores. Note: We enforce a minLen here, to get rid of spurious very low-length sequences. This was not needed in the 16S Tutorial Workflow because truncLen already served that purpose.

```{r}
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 2), 
    truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
head(out)
```

``` {r}
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
```

Visualize the estimated error rates as a sanity check

``` {r}
plotErrors(errF, nominalQ = TRUE)
```
## Dereplicate identical reads 

``` {r}
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names.f
names(derepRs) <- sample.names.r
```

## Sample Inference 

what is the core sample inference algorithm? 

```{r}
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)
```

## Merge Paired Reads

```{r} 
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
```

## Construct Sequence Table 

We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

This is different than the manual because we deleted three files 

## Remove Chimeras 

**understand chimeras** 

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
```

Inspect distribution of sequence lengths:
```{r}
table(nchar(getSequences(seqtab.nochim)))
```
As expected, quite a bit of length variability in the the amplified ITS region.

## Track Reads Through the Pipeline 

We now inspect the the number of reads that made it through each step in the pipeline to verify everything worked as expected. If it seems like a major amount was lost in any step, something could have gone wrong. 

```{r} 
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, 
    getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace
# sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", 
    "nonchim")
rownames(track) <- sample.names
head(track)
```

## Assign Taxonomy 

DADA2 supports fungal taxonmic assignment using the UNITE database! The DADA2 package provides a native implementation of the naive Bayesian classifier method for taxonomic assignment. The assignTaxonomy function takes as input a set of sequences to ba classified, and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least minBoot bootstrap confidence. For fungal taxonomy, the General Fasta release files from the UNITE ITS database can be downloaded and used as the reference.

```{r}
unite.ref <- "~/Tabima_lab/homes/mhincher/Tabima_lab/raw_data/watershed/fecal/sh_general_release_dynamic_16.10.2022.fasta"  # CHANGE ME to location on your machine
taxa <- assignTaxonomy(seqtab.nochim, unite.ref, multithread = TRUE, tryRC = TRUE)
```

Inspecting taxanomic assignments

```{r}
taxa.print <- taxa  # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```
